{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPjqp/Mb9KlyxiUH9bteTcz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"qFdPvlXBOdUN"},"source":["# Introduction to Mixed precision\n","Adapted from the mixed precision guide at https://www.tensorflow.org/guide/mixed_precision"]},{"cell_type":"markdown","metadata":{"id":"xHxb-dlhMIzW"},"source":["## Overview\n","\n","Mixed precision is the use of both half precision (float16) and single precision (float32) floating-point types in a model during training to make it run faster and use less memory. By keeping certain parts of the model in the **single precision types for numeric stability**, and some parts in the **half precision type for increased compute and memory throughput and also Tensor Core advantage**, the model will have a lower step time and train equally as well in terms of the evaluation metrics such as accuracy. \n","\n","This guide describes how to use the experimental Keras mixed precision API to speed up your models. Using this API can improve performance by more than **3 times on modern NVIDIA GPUs**."]},{"cell_type":"markdown","metadata":{"id":"AEDHaeuR7kLX"},"source":["Note: The Keras mixed precision API is currently experimental and may change."]},{"cell_type":"markdown","metadata":{"id":"3vsYi_bv7gS_"},"source":["Today, most models use the float32 dtype, which takes 32 bits of memory. However, there are lower-precision dtypes, float16 which take 16 bits of memory instead. Modern accelerators can run operations faster in the 16-bit dtypes, as they have specialized hardware to run 16-bit computations and 16-bit dtypes can be read from memory faster.\n","\n","NVIDIA GPUs can run operations in float16 faster than in float32. Therefore, these lower-precision dtypes should be used whenever possible on those devices. However, variables and a few computations should still be in float32 for numeric reasons so that the model trains to the same quality. The Keras mixed precision API allows you to use float16 to get the performance benefits from float16 and the numeric stability benefits from float32.\n","\n","Note: The term \"numeric stability\" refers to how a model's quality is affected by the use of a lower-precision dtype instead of a higher precision dtype. We say an operation is \"numerically unstable\" in float16 if running it in one of those dtypes causes the model to have worse evaluation accuracy or other metrics compared to running the operation in float32."]},{"cell_type":"markdown","metadata":{"id":"MUXex9ctTuDB"},"source":["## Setup"]},{"cell_type":"code","metadata":{"id":"IqR2PQG4ZaZ0","executionInfo":{"status":"ok","timestamp":1606789991790,"user_tz":-480,"elapsed":2204,"user":{"displayName":"Zhangsheng Lai SG","photoUrl":"","userId":"12556235241211667079"}}},"source":["import tensorflow as tf\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.mixed_precision import experimental as mixed_precision"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"814VXqdh8Q0r"},"source":["## Supported hardware\n","\n","While mixed precision will run on most hardware, it will only speed up models on recent NVIDIA GPUs. NVIDIA GPUs support using a mix of float16 and float32.\n","\n","Among NVIDIA GPUs, those with compute capability 7.0 or higher will see the greatest performance benefit from mixed precision because they have special hardware units, called Tensor Cores, to accelerate float16 matrix multiplications and convolutions. Older GPUs offer no math performance benefit for using mixed precision, however memory and bandwidth savings can enable some speedups. You can look up the compute capability for your GPU at NVIDIA's [CUDA GPU web page](https://developer.nvidia.com/cuda-gpus). Examples of GPUs that will benefit most from mixed precision include RTX GPUs, the Titan V, the V100 and of course the A100s."]},{"cell_type":"markdown","metadata":{"id":"-q2hisD60F0_"},"source":["You can check your GPU type with the following. The command only exists if the\n","NVIDIA drivers are installed, so the following will raise an error otherwise."]},{"cell_type":"code","metadata":{"id":"j-Yzg_lfkoa_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606789991790,"user_tz":-480,"elapsed":836,"user":{"displayName":"Zhangsheng Lai SG","photoUrl":"","userId":"12556235241211667079"}},"outputId":"6c567892-3be2-42a4-952f-a62eb20e6077"},"source":["!nvidia-smi -L"],"execution_count":2,"outputs":[{"output_type":"stream","text":["GPU 0: Tesla T4 (UUID: GPU-c4a1f8d8-7429-a480-e77b-f82865904c92)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HNOmvumB-orT"},"source":["## Setting the dtype policy"]},{"cell_type":"markdown","metadata":{"id":"54ecYY2Hn16E"},"source":["To use mixed precision in Keras, you need to create a `tf.keras.mixed_precision.experimental.Policy`, typically referred to as a *dtype policy*. Dtype policies specify the dtypes layers will run in. In this guide, you will construct a policy from the string `'mixed_float16'` and set it as the global policy. This will will cause subsequently created layers to use mixed precision with a mix of float16 and float32."]},{"cell_type":"code","metadata":{"id":"x3kElPVH-siO","executionInfo":{"status":"ok","timestamp":1606790043676,"user_tz":-480,"elapsed":1111,"user":{"displayName":"Zhangsheng Lai SG","photoUrl":"","userId":"12556235241211667079"}}},"source":["# policy = mixed_precision.Policy('mixed_float16')\n","policy = mixed_precision.Policy('float32')\n","mixed_precision.set_policy(policy)"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oGAMaa0Ho3yk"},"source":["The policy specifies two important aspects of a layer: the dtype the layer's computations are done in, and the dtype of a layer's variables. Above, you created a `mixed_float16` policy (i.e., a `mixed_precision.Policy` created by passing the string `'mixed_float16'` to its constructor). With this policy, layers use float16 computations and float32 variables. Computations are done in float16 for performance, but variables must be kept in float32 for numeric stability. You can directly query these properties of the policy."]},{"cell_type":"code","metadata":{"id":"GQRbYm4f8p-k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606790044176,"user_tz":-480,"elapsed":1286,"user":{"displayName":"Zhangsheng Lai SG","photoUrl":"","userId":"12556235241211667079"}},"outputId":"cb52074a-a986-49d9-c577-82430b27562c"},"source":["print('Compute dtype: %s' % policy.compute_dtype)\n","print('Variable dtype: %s' % policy.variable_dtype)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Compute dtype: float32\n","Variable dtype: float32\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MOFEcna28o4T"},"source":["As mentioned before, the `mixed_float16` policy will most significantly improve performance on NVIDIA GPUs with compute capability of at least 7.0. The policy will run on other GPUs and CPUs but may not improve performance."]},{"cell_type":"markdown","metadata":{"id":"cAHpt128tVpK"},"source":["## Building the model"]},{"cell_type":"markdown","metadata":{"id":"nB6ujaR8qMAy"},"source":["Next, let's start building a simple model. Very small toy models typically do not benefit from mixed precision, because overhead from the TensorFlow runtime  typically dominates the execution time, making any performance improvement on the GPU negligible. Therefore, let's build two large `Dense` layers with 4096 units each if a GPU is used."]},{"cell_type":"code","metadata":{"id":"0DQM24hL_14Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606790044463,"user_tz":-480,"elapsed":970,"user":{"displayName":"Zhangsheng Lai SG","photoUrl":"","userId":"12556235241211667079"}},"outputId":"da1761c2-45f9-4f8a-a5a2-3353dcf74a3c"},"source":["inputs = keras.Input(shape=(784,), name='digits')\n","if tf.config.list_physical_devices('GPU'):\n","  print('The model will run with 4096 units on a GPU')\n","  num_units = 4096\n","else:\n","  # Use fewer units on CPUs so the model finishes in a reasonable amount of time\n","  print('The model will run with 64 units on a CPU')\n","  num_units = 64\n","dense1 = layers.Dense(num_units, activation='relu', name='dense_1')\n","x = dense1(inputs)\n","dense2 = layers.Dense(num_units, activation='relu', name='dense_2')\n","x = dense2(x)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["The model will run with 4096 units on a GPU\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2dezdcqnOXHk"},"source":["Each layer has a policy and uses the global policy by default. Each of the `Dense` layers therefore have the `mixed_float16` policy because you set the global policy to `mixed_float16` previously. This will cause the dense layers to do float16 computations and have float32 variables. They cast their inputs to float16 in order to do float16 computations, which causes their outputs to be float16 as a result. Their variables are float32 and will be cast to float16 when the layers are called to avoid errors from dtype mismatches."]},{"cell_type":"code","metadata":{"id":"kC58MzP4PEcC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606790044748,"user_tz":-480,"elapsed":919,"user":{"displayName":"Zhangsheng Lai SG","photoUrl":"","userId":"12556235241211667079"}},"outputId":"5dbc6918-25e4-4f01-be61-6fbb0643ae13"},"source":["print('x.dtype: %s' % x.dtype.name)\n","# 'kernel' is dense1's variable\n","print('dense1.kernel.dtype: %s' % dense1.kernel.dtype.name)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["x.dtype: float32\n","dense1.kernel.dtype: float32\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_WAZeqDyqZcb"},"source":["Next, create the output predictions. Normally, you can create the output predictions as follows, but this is not always numerically stable with float16."]},{"cell_type":"code","metadata":{"id":"ybBq1JDwNIbz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606790045173,"user_tz":-480,"elapsed":914,"user":{"displayName":"Zhangsheng Lai SG","photoUrl":"","userId":"12556235241211667079"}},"outputId":"c3e2d7eb-1f89-48a4-8048-658090426811"},"source":["# INCORRECT: softmax and model output will be float16, when it should be float32\n","outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n","print('Outputs dtype: %s' % outputs.dtype.name)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Outputs dtype: float32\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"D0gSWxc9NN7q"},"source":["A softmax activation at the end of the model should be float32. Because the dtype policy is `mixed_float16`, the softmax activation would normally have a float16 compute dtype and output a float16 tensors.\n","\n","This can be fixed by separating the Dense and softmax layers, and by passing `dtype='float32'` to the softmax layer"]},{"cell_type":"code","metadata":{"id":"IGqCGn4BsODw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606790046363,"user_tz":-480,"elapsed":650,"user":{"displayName":"Zhangsheng Lai SG","photoUrl":"","userId":"12556235241211667079"}},"outputId":"5c49043c-a340-41ad-b476-a5bb25f173a8"},"source":["# CORRECT: softmax and model output are float32\n","x = layers.Dense(10, name='dense_logits')(x)\n","outputs = layers.Activation('softmax', dtype='float32', name='predictions')(x)\n","print('Outputs dtype: %s' % outputs.dtype.name)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Outputs dtype: float32\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tUdkY_DHsP8i"},"source":["Passing `dtype='float32'` to the softmax layer constructor overrides the layer's dtype policy to be the `float32` policy, which does computations and keeps variables in float32. Equivalently, we could have instead passed `dtype=mixed_precision.Policy('float32')`; layers always convert the dtype argument to a policy. Because the `Activation` layer has no variables, the policy's variable dtype is ignored, but the policy's compute dtype of float32 causes softmax and the model output to be float32. \n","\n","\n","Adding a float16 softmax in the middle of a model is fine, but a softmax at the end of the model should be in float32. The reason is that if the intermediate tensor flowing from the softmax to the loss is float16, numeric issues may occur.\n","\n","You can override the dtype of any layer to be float32 by passing `dtype='float32'` if you think it will not be numerically stable with float16 computations. But typically, this is only necessary on the last layer of the model, as most layers have sufficient precision with `mixed_float16`."]},{"cell_type":"markdown","metadata":{"id":"_8ZsftkmFXhE"},"source":["In the case where a model does not end in a softmax, the outputs should still be float32. While unnecessary for this specific model, the model outputs can be cast to float32 with the following:"]},{"cell_type":"code","metadata":{"id":"dzVAoLI56jR8","executionInfo":{"status":"ok","timestamp":1606790048710,"user_tz":-480,"elapsed":774,"user":{"displayName":"Zhangsheng Lai SG","photoUrl":"","userId":"12556235241211667079"}}},"source":["# The linear activation is an identity function. So this simply casts 'outputs'\n","# to float32. In this particular case, 'outputs' is already float32 so this is a\n","# no-op.\n","# outputs = layers.Activation('linear', dtype='float32')(outputs)"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tpY4ZP7us5hA"},"source":["Next, finish and compile the model, and generate input data."]},{"cell_type":"code","metadata":{"id":"g4OT3Z6kqYAL","executionInfo":{"status":"ok","timestamp":1606790051142,"user_tz":-480,"elapsed":1517,"user":{"displayName":"Zhangsheng Lai SG","photoUrl":"","userId":"12556235241211667079"}}},"source":["model = keras.Model(inputs=inputs, outputs=outputs)\n","model.compile(loss='sparse_categorical_crossentropy',\n","              optimizer=keras.optimizers.RMSprop(),\n","              metrics=['accuracy'])\n","\n","(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n","x_train = x_train.reshape(60000, 784).astype('float32') / 255\n","x_test = x_test.reshape(10000, 784).astype('float32') / 255"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hhrX90asGCBs","executionInfo":{"status":"ok","timestamp":1606790001059,"user_tz":-480,"elapsed":845,"user":{"displayName":"Zhangsheng Lai SG","photoUrl":"","userId":"12556235241211667079"}},"outputId":"4c762923-9160-49a1-b213-28425f6ea636"},"source":["model.summary()"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Model: \"functional_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","digits (InputLayer)          [(None, 784)]             0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 4096)              3215360   \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 4096)              16781312  \n","_________________________________________________________________\n","dense_logits (Dense)         (None, 10)                40970     \n","_________________________________________________________________\n","predictions (Activation)     (None, 10)                0         \n","=================================================================\n","Total params: 20,037,642\n","Trainable params: 20,037,642\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0Sm8FJHegVRN"},"source":["This example cast the input data from int8 to float32. We don't cast to float16 since the division by 255 is on the CPU, which runs float16 operations slower than float32 operations. In this case, the performance difference in negligible, but in general you should run input processing math in float32 if it runs on the CPU. The first layer of the model will cast the inputs to float16, as each layer casts floating-point inputs to its compute dtype.\n","\n","The initial weights of the model are retrieved. This will allow training from scratch again by loading the weights."]},{"cell_type":"code","metadata":{"id":"0UYs-u_DgiA5","executionInfo":{"status":"ok","timestamp":1606790002136,"user_tz":-480,"elapsed":813,"user":{"displayName":"Zhangsheng Lai SG","photoUrl":"","userId":"12556235241211667079"}}},"source":["initial_weights = model.get_weights()"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zlqz6eVKs9aU"},"source":["## Training the model with Model.fit\n","\n","Next, train the model."]},{"cell_type":"code","metadata":{"id":"hxI7-0ewmC0A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606790032575,"user_tz":-480,"elapsed":3538,"user":{"displayName":"Zhangsheng Lai SG","photoUrl":"","userId":"12556235241211667079"}},"outputId":"0aebe50d-9b09-4b36-d9cf-dc88d81e7854"},"source":["# dtype policy mixed_float16\n","history = model.fit(x_train, y_train,\n","                    batch_size=8192,\n","                    epochs=5,\n","                    validation_split=0.2)\n","test_scores = model.evaluate(x_test, y_test, verbose=2)\n","print('Test loss:', test_scores[0])\n","print('Test accuracy:', test_scores[1])\n"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","6/6 [==============================] - 0s 67ms/step - loss: 0.2890 - accuracy: 0.9103 - val_loss: 0.1471 - val_accuracy: 0.9577\n","Epoch 2/5\n","6/6 [==============================] - 0s 60ms/step - loss: 0.2292 - accuracy: 0.9338 - val_loss: 0.3089 - val_accuracy: 0.9073\n","Epoch 3/5\n","6/6 [==============================] - 0s 59ms/step - loss: 0.2652 - accuracy: 0.9142 - val_loss: 0.1344 - val_accuracy: 0.9608\n","Epoch 4/5\n","6/6 [==============================] - 0s 60ms/step - loss: 0.1129 - accuracy: 0.9670 - val_loss: 0.1491 - val_accuracy: 0.9532\n","Epoch 5/5\n","6/6 [==============================] - 0s 59ms/step - loss: 0.2116 - accuracy: 0.9341 - val_loss: 0.2529 - val_accuracy: 0.9189\n","313/313 - 1s - loss: 0.2627 - accuracy: 0.9208\n","Test loss: 0.26267552375793457\n","Test accuracy: 0.920799970626831\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CPLns7PyF3Gx","executionInfo":{"status":"ok","timestamp":1606790076128,"user_tz":-480,"elapsed":10058,"user":{"displayName":"Zhangsheng Lai SG","photoUrl":"","userId":"12556235241211667079"}},"outputId":"ae299c0e-333a-4006-afa4-644a6bc4d7db"},"source":["# dtype policy float32\n","history = model.fit(x_train, y_train,\n","                    batch_size=8192,\n","                    epochs=5,\n","                    validation_split=0.2)\n","test_scores = model.evaluate(x_test, y_test, verbose=2)\n","print('Test loss:', test_scores[0])\n","print('Test accuracy:', test_scores[1])\n"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","2/6 [=========>....................] - ETA: 0s - loss: 0.1598 - accuracy: 0.9553WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0906s vs `on_train_batch_end` time: 0.1671s). Check your callbacks.\n","6/6 [==============================] - 2s 272ms/step - loss: 0.2383 - accuracy: 0.9300 - val_loss: 0.3785 - val_accuracy: 0.8758\n","Epoch 2/5\n","6/6 [==============================] - 2s 270ms/step - loss: 0.2224 - accuracy: 0.9336 - val_loss: 0.1267 - val_accuracy: 0.9624\n","Epoch 3/5\n","6/6 [==============================] - 2s 267ms/step - loss: 0.3078 - accuracy: 0.9153 - val_loss: 0.2727 - val_accuracy: 0.9234\n","Epoch 4/5\n","6/6 [==============================] - 2s 267ms/step - loss: 0.1480 - accuracy: 0.9594 - val_loss: 0.1162 - val_accuracy: 0.9644\n","Epoch 5/5\n","6/6 [==============================] - 2s 269ms/step - loss: 0.1736 - accuracy: 0.9440 - val_loss: 0.3850 - val_accuracy: 0.8767\n","313/313 - 1s - loss: 0.4063 - accuracy: 0.8672\n","Test loss: 0.40625640749931335\n","Test accuracy: 0.8672000169754028\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MPhJ9OPWt4x5"},"source":["Notice the model prints the time per step in the logs: for example, \"70ms/step\". The first epoch may be slower as TensorFlow spends some time optimizing the model, but afterwards the time per sample should stabilize. \n","\n","If you are running this guide in Colab, you can compare the performance of mixed precision with float32. To do so, change the policy from `mixed_float16` to `float32` in the \"Setting the dtype policy\" section, then rerun all the cells up to this point. On GPUs with at least compute capability 7.0, you should see the time per sample significantly increase, indicating mixed precision sped up the model. For example, with a Titan V GPU, the per-sample time increases from 4us to 12us. Make sure to change the policy back to `mixed_float16` and rerun the cells before continuing with the guide.\n","\n","For many real-world models, mixed precision also allows you to double the batch size without running out of memory, as float16 tensors take half the memory. This does not apply however to this toy model, as you can likely run the model in any dtype where each batch consists of the entire MNIST dataset of 60,000 images."]},{"cell_type":"markdown","metadata":{"id":"mNKMXlCvHgHb"},"source":["## Loss scaling\n","\n","Loss scaling is a technique which `tf.keras.Model.fit` automatically performs with the `mixed_float16` policy to avoid numeric underflow. This section describes loss scaling and how to customize its behavior."]},{"cell_type":"markdown","metadata":{"id":"1xQX62t2ow0g"},"source":["### Underflow and Overflow\n","\n","The float16 data type has a narrow dynamic range compared to float32. This means values above $65504$ will overflow to infinity and values below $6.0 \\times 10^{-8}$ will underflow to zero. float32 and bfloat16 have a much higher dynamic range so that overflow and underflow are not a problem.\n","\n","For example:"]},{"cell_type":"code","metadata":{"id":"CHmXRb-yRWbE"},"source":["x = tf.constant(256, dtype='float16')\n","(x ** 2).numpy()  # Overflow"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5unZLhN0RfQM"},"source":["x = tf.constant(1e-5, dtype='float16')\n","(x ** 2).numpy()  # Underflow"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pUIbhQypRVe_"},"source":["In practice, overflow with float16 rarely occurs. Additionally, underflow also rarely occurs during the forward pass. However, during the backward pass, gradients can underflow to zero. Loss scaling is a technique to prevent this underflow."]},{"cell_type":"markdown","metadata":{"id":"FAL5qij_oNqJ"},"source":["### Loss scaling background\n","\n","The basic concept of loss scaling is simple: simply multiply the loss by some large number, say $1024$. We call this number the *loss scale*. This will cause the gradients to scale by $1024$ as well, greatly reducing the chance of underflow. Once the final gradients are computed, divide them by $1024$ to bring them back to their correct values.\n","\n","The pseudocode for this process is:\n","\n","```\n","loss_scale = 1024\n","loss = model(inputs)\n","loss *= loss_scale\n","# We assume `grads` are float32. We do not want to divide float16 gradients\n","grads = compute_gradient(loss, model.trainable_variables)\n","grads /= loss_scale\n","```\n","\n","Choosing a loss scale can be tricky. If the loss scale is too low, gradients may still underflow to zero. If too high, the opposite the problem occurs: the gradients may overflow to infinity.\n","\n","To solve this, TensorFlow dynamically determines the loss scale so you do not have to choose one manually. If you use `tf.keras.Model.fit`, loss scaling is done for you so you do not have to do any extra work. This is explained further in the next section.\n"]},{"cell_type":"markdown","metadata":{"id":"qlbxbjxShf0m"},"source":["### Choosing the loss scale\n","\n","Each dtype policy optionally has an associated `tf.mixed_precision.experimental.LossScale` object, which represents a fixed or dynamic loss scale. By default, the loss scale for the `mixed_float16` policy is a `tf.mixed_precision.experimental.DynamicLossScale`, which dynamically determines the loss scale value. Other policies do not have a loss scale by default, as it is only necessary when float16 is used. You can query the loss scale of the policy:"]},{"cell_type":"code","metadata":{"id":"0iSbassjdldy"},"source":["loss_scale = policy.loss_scale\n","print('Loss scale: %s' % loss_scale)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l8c0ULUxdu2b"},"source":["The loss scale prints a lot of internal state, but you can ignore it. The most important part is the `current_loss_scale` part, which shows the loss scale's current value."]},{"cell_type":"markdown","metadata":{"id":"Mr0cAMCHfhlj"},"source":["You can instead use a static loss scale by passing a number when constructing a dtype policy."]},{"cell_type":"code","metadata":{"id":"4Sa4bGFif9GW"},"source":["new_policy = mixed_precision.Policy('mixed_float16', loss_scale=1024)\n","print(new_policy.loss_scale)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"alD_0frtgFKK"},"source":["The dtype policy constructor always converts the loss scale to a `LossScale` object. In this case, it's converted to a `tf.mixed_precision.experimental.FixedLossScale`, the only other `LossScale` subclass other than `DynamicLossScale`."]},{"cell_type":"markdown","metadata":{"id":"Gi1DLrLF7bnr"},"source":["Note: *Using anything other than a dynamic loss scale is not recommended*. Choosing a fixed loss scale can be difficult, as making it too low will cause the model to not train as well, and making it too high will cause Infs or NaNs to appear in the gradients. A dynamic loss scale is typically near the optimal loss scale, so you do not have to do any work. Currently, dynamic loss scales are a bit slower than fixed loss scales, but the performance will be improved in the future."]},{"cell_type":"markdown","metadata":{"id":"LJ_1W-Axh2bp"},"source":["Models, like layers, each have a dtype policy. If present, a model uses its policy's loss scale to apply loss scaling in the `tf.keras.Model.fit` method. This means if `Model.fit` is used, you do not have to worry about loss scaling at all: The `mixed_float16` policy will have a dynamic loss scale by default, and `Model.fit` will apply it."]},{"cell_type":"markdown","metadata":{"id":"YuMh0-wnFXhE"},"source":["## GPU performance tips\n","\n","Here are some performance tips when using mixed precision on GPUs.\n","\n","### Increasing your batch size\n","If it doesn't affect model quality, try running with double the batch size when using mixed precision. As float16 tensors use half the memory, this often allows you to double your batch size without running out of memory. Increasing batch size typically increases training throughput, i.e. the training elements per second your model can run on.\n","\n","### Ensuring GPU Tensor Cores are used\n","\n","As mentioned previously, modern NVIDIA GPUs use a special hardware unit called Tensor Cores that can multiply float16 matrices very quickly. However, Tensor Cores requires certain dimensions of tensors to be a multiple of 8. In the examples below, an argument is bold if and only if it needs to be a multiple of 8 for Tensor Cores to be used.\n","\n","* tf.keras.layers.Dense(**units=64**)\n","* tf.keras.layers.Conv2d(**filters=48**, kernel_size=7, stride=3)\n","  * And similarly for other convolutional layers, such as tf.keras.layers.Conv3d\n","* tf.keras.layers.LSTM(**units=64**)\n","  * And similar for other RNNs, such as tf.keras.layers.GRU\n","* tf.keras.Model.fit(epochs=2, **batch_size=128**)\n","\n","You should try to use Tensor Cores when possible. If you want to learn more [NVIDIA deep learning performance guide](https://docs.nvidia.com/deeplearning/sdk/dl-performance-guide/index.html) describes the exact requirements for using Tensor Cores as well as other Tensor Core-related performance information."]},{"cell_type":"markdown","metadata":{"id":"--wSEU91wO9w"},"source":["## Summary\n","* You should use mixed precision if you use TPUs or NVIDIA GPUs with at least compute capability 7.0, as it will improve performance by up to 3x.\n","* You can use mixed precision with the following lines:\n","  ```\n","  policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n","  mixed_precision.set_policy(policy)\n","  ```\n","* If your model ends in softmax, make sure it is float32. And regardless of what your model ends in, make sure the output is float32.\n","* Double the training batch size if it does not reduce evaluation accuracy\n","* On GPUs, ensure most tensor dimensions are a multiple of $8$ to maximize performance\n","\n","For more examples of mixed precision using the `tf.keras.mixed_precision` API, see the [official models repository](https://github.com/tensorflow/models/tree/master/official). Most official models, such as [ResNet](https://github.com/tensorflow/models/tree/master/official/vision/image_classification) and [Transformer](https://github.com/tensorflow/models/blob/master/official/nlp/transformer) will run using mixed precision by passing `--dtype=fp16`. "]}]}